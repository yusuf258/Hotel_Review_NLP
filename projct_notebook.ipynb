{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Otel YorumlarÄ± Duygu Analizi - DL (LSTM)\n",
    "Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otel Yorumu Duygu Analizi | LSTM Sentiment Analysis\n",
    "\n",
    "| Ã–zellik | Detay |\n",
    "|---|---|\n",
    "| **GÃ¶rev** | 3-SÄ±nÄ±flÄ± Duygu Analizi (Negative / Neutral / Positive) |\n",
    "| **Veri** | Hotel yorumlarÄ± â€” Ä°ngilizce kullanÄ±cÄ± yorumlarÄ± (Rating: 1â€“5) |\n",
    "| **Ã–n Ä°ÅŸleme** | Dil filtresi + Tokenize + Stopword kaldÄ±rma + Lemmatizasyon |\n",
    "| **Ã–rnekleme** | RandomOverSampler (neutral â†‘) + RandomUnderSampler (positive â†“) |\n",
    "| **Model** | LSTM: Embedding(10k, 128) + SpatialDropout + LSTM(128) + Dense(3) |\n",
    "| **DoÄŸruluk** | Test Accuracy = 0.77 (%77) |\n",
    "| **Teknoloji** | TensorFlow / Keras, NLTK, imbalanced-learn |\n",
    "\n",
    "**SÄ±nÄ±f Dengesi:** Positive aÄŸÄ±rlÄ±klÄ± veri (%73.7); oversampling/undersampling ile eÄŸitim seti dengelenmiÅŸtir.\n",
    "**En ZayÄ±f SÄ±nÄ±f:** Neutral (F1 = 0.29) â€” 3 puanlÄ± yorumlarÄ±n belirsiz sÄ±nÄ±r niteliÄŸi sÄ±nÄ±flandÄ±rmayÄ± zorlaÅŸtÄ±rmaktadÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerekli KÃ¼tÃ¼phanelerin YÃ¼klenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Proje kÃ¶k dizinini ekleme (src modÃ¼lÃ¼ hatasÄ±nÄ± Ã¶nlemek iÃ§in)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "   \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from src.data_preprocessing import preprocess_reviews, clean_text, create_sentiment_label\n",
    "from langdetect import detect\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Veri YÃ¼kleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri setinin ilk 5 satÄ±rÄ±:\n",
      "                                              Review  Rating\n",
      "0  nice hotel expensive parking got good deal sta...       4\n",
      "1  ok nothing special charge diamond member hilto...       2\n",
      "2  nice rooms not 4* experience hotel monaco seat...       3\n",
      "3  unique, great stay, wonderful time hotel monac...       5\n",
      "4  great stay great stay, went seahawk game aweso...       5\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "print(\"Veri setinin ilk 5 satÄ±rÄ±:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dil Tespiti ve Filtreleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä°ngilizce olmayan yorumlar filtrelendi.\n"
     ]
    }
   ],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "df['lang'] = df['Review'].apply(detect_lang)\n",
    "df = df[df['lang'] == 'en']\n",
    "print(\"Ä°ngilizce olmayan yorumlar filtrelendi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metin Ã–n Ä°ÅŸleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã–n iÅŸlenmiÅŸ verinin ilk 5 satÄ±rÄ±:\n",
      "                                              Review  \\\n",
      "0  nice hotel expensive parking got good deal sta...   \n",
      "1  ok nothing special charge diamond member hilto...   \n",
      "2  nice rooms not 4* experience hotel monaco seat...   \n",
      "3  unique, great stay, wonderful time hotel monac...   \n",
      "4  great stay great stay, went seahawk game aweso...   \n",
      "\n",
      "                                    processed_review sentiment  \n",
      "0  nice hotel expensive parking got good deal sta...  positive  \n",
      "1  ok nothing special charge diamond member hilto...  negative  \n",
      "2  nice room experience hotel monaco seattle good...   neutral  \n",
      "3  unique great stay wonderful time hotel monaco ...  positive  \n",
      "4  great stay great stay went seahawk game awesom...  positive  \n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def apply_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def create_sentiment_label(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 'negative'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    elif rating in [4, 5]:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_reviews(df, text_column='Review', rating_column='Rating'):\n",
    "    df['cleaned_review'] = df[text_column].apply(clean_text)\n",
    "    df['tokens'] = df['cleaned_review'].apply(tokenize_text)\n",
    "    df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "    df['lemmatized_tokens'] = df['filtered_tokens'].apply(apply_lemmatization)\n",
    "    df['processed_review'] = df['lemmatized_tokens'].apply(lambda x: ' '.join(x))\n",
    "    df['sentiment'] = df[rating_column].apply(create_sentiment_label)\n",
    "    return df\n",
    "\n",
    "# Ã–n iÅŸleme adÄ±mlarÄ±nÄ± uygulama\n",
    "df_processed = preprocess_reviews(df.copy())\n",
    "\n",
    "print(\"Ã–n iÅŸlenmiÅŸ verinin ilk 5 satÄ±rÄ±:\")\n",
    "print(df_processed[['Review', 'processed_review', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['processed_review'] = df['Review'].apply(clean_text)\n",
    "#df['sentiment'] = df['Rating'].apply(create_sentiment_label)\n",
    "#df.dropna(subset=['sentiment'], inplace=True)\n",
    "#print(\"Ã–n iÅŸlenmiÅŸ verinin ilk 5 satÄ±rÄ±:\")\n",
    "#print(df[['Review', 'processed_review', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train / Test BÃ¶lme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EÄŸitim seti boyutu: (16380,)\n",
      "Etiketler dÃ¼z metin olarak kaldÄ±: 3332      neutral\n",
      "5970     positive\n",
      "10394    positive\n",
      "8603     positive\n",
      "10515    positive\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = df_processed['processed_review']\n",
    "# DÄ°KKAT: Burada get_dummies kullanmÄ±yoruz, ham etiketleri alÄ±yoruz\n",
    "y = df_processed['sentiment']\n",
    "\n",
    "# Stratify parametresi dengeli daÄŸÄ±lÄ±m iÃ§in Ã¶nemlidir\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"EÄŸitim seti boyutu:\", X_train.shape)\n",
    "print(\"Etiketler dÃ¼z metin olarak kaldÄ±:\", y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_processed['processed_review']\n",
    "#y = pd.get_dummies(df_processed['sentiment'])\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metinlerin Dizilere DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_len = 150\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SÄ±nÄ±f DengesizliÄŸi Giderme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SÄ±nÄ±f dengesizliÄŸini gidermek iÃ§in `RandomOverSampler` ve `RandomUnderSampler` kullanÄ±lacaktÄ±r. `RandomOverSampler` ile 'neutral' sÄ±nÄ±fÄ± artÄ±rÄ±lacak, `RandomUnderSampler` ile 'positive' sÄ±nÄ±fÄ± azaltÄ±lacaktÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã–rnekleme sonrasÄ± veri seti boyutu: (17568, 150)\n",
      "One-Hot dÃ¶nÃ¼ÅŸÃ¼mÃ¼ tamamlandÄ±.\n",
      "   negative  neutral  positive\n",
      "0      True    False     False\n",
      "1      True    False     False\n",
      "2      True    False     False\n",
      "3      True    False     False\n",
      "4      True    False     False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Not: y_train artÄ±k dÃ¼z metin olduÄŸu iÃ§in sampling_strategy Ã§alÄ±ÅŸacaktÄ±r.\n",
    "over_sampler = RandomOverSampler(sampling_strategy={'neutral': 5000})\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={'positive': 10000})\n",
    "\n",
    "# 1. Ã–nce DengesizliÄŸi Gider (Resampling)\n",
    "X_train_res, y_train_res = over_sampler.fit_resample(X_train_pad, y_train)\n",
    "X_train_res, y_train_res = under_sampler.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "print(\"Ã–rnekleme sonrasÄ± veri seti boyutu:\", X_train_res.shape)\n",
    "\n",
    "# 2. ÅÄ°MDÄ° One-Hot Encoding'e Ã‡evir (LSTM iÃ§in gerekli)\n",
    "y_train_res = pd.get_dummies(y_train_res)\n",
    "y_test = pd.get_dummies(y_test) # Test setini de Ã§evirmeyi unutmayÄ±n!\n",
    "\n",
    "# SÃ¼tun sÄ±rasÄ±nÄ±n aynÄ± olduÄŸundan emin olalÄ±m (neg, neu, pos)\n",
    "y_train_res = y_train_res[['negative', 'neutral', 'positive']]\n",
    "y_test = y_test[['negative', 'neutral', 'positive']]\n",
    "\n",
    "print(\"One-Hot dÃ¶nÃ¼ÅŸÃ¼mÃ¼ tamamlandÄ±.\")\n",
    "print(y_train_res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over_sampler = RandomOverSampler(sampling_strategy={'neutral': 5000})\n",
    "#under_sampler = RandomUnderSampler(sampling_strategy={'positive': 10000})\n",
    "#X_train_res, y_train_res = over_sampler.fit_resample(X_train_pad, y_train)\n",
    "#X_train_res, y_train_res = under_sampler.fit_resample(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model TanÄ±mÄ± (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model EÄŸitimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m248/248\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 304ms/step - accuracy: 0.6697 - loss: 0.7304 - val_accuracy: 0.8867 - val_loss: 0.3281\n",
      "Epoch 2/5\n",
      "\u001b[1m248/248\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 297ms/step - accuracy: 0.8352 - loss: 0.4349 - val_accuracy: 0.7997 - val_loss: 0.4387\n",
      "Epoch 3/5\n",
      "\u001b[1m248/248\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 309ms/step - accuracy: 0.8851 - loss: 0.3216 - val_accuracy: 0.8702 - val_loss: 0.3275\n",
      "Epoch 4/5\n",
      "\u001b[1m248/248\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 325ms/step - accuracy: 0.9143 - loss: 0.2426 - val_accuracy: 0.8987 - val_loss: 0.2660\n",
      "Epoch 5/5\n",
      "\u001b[1m248/248\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 319ms/step - accuracy: 0.9431 - loss: 0.1707 - val_accuracy: 0.8873 - val_loss: 0.3114\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "history = model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. DeÄŸerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m128/128\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step\n",
      "SÄ±nÄ±flandÄ±rma Raporu:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.56      0.61       642\n",
      "     neutral       0.25      0.35      0.29       437\n",
      "    positive       0.90      0.88      0.89      3016\n",
      "\n",
      "    accuracy                           0.77      4095\n",
      "   macro avg       0.61      0.59      0.60      4095\n",
      "weighted avg       0.79      0.77      0.78      4095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_pad)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test.values, axis=1)\n",
    "report = classification_report(y_test_classes, y_pred_classes, target_names=['negative', 'neutral', 'positive'])\n",
    "print(\"SÄ±nÄ±flandÄ±rma Raporu:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Modeli Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ve tokenizer '../data/processed' dizinine kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save(os.path.join(output_dir, 'sentiment_model_dl.h5'))\n",
    "joblib.dump(tokenizer, os.path.join(output_dir, 'tokenizer_dl.pkl'))\n",
    "print(f\"Model ve tokenizer '{output_dir}' dizinine kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Genel Performans Ã–zeti**\n",
    "Accuracy: 0.77 (%77)\n",
    "Bu deÄŸer, ilk bakÄ±ÅŸta makul gÃ¶rÃ¼nse de veri setinin pozitif aÄŸÄ±rlÄ±klÄ± yapÄ±sÄ± nedeniyle tek baÅŸÄ±na yeterli deÄŸildir.\n",
    "SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±:\n",
    "Positive: 3016 (%73.7)\n",
    "Negative: 642 (%15.7)\n",
    "Neutral: 437 (%10.6)\n",
    "\n",
    "Bu dengesizlik, accuracyâ€™nin olduÄŸundan daha iyi gÃ¶rÃ¼nmesine neden oluyor.\n",
    "\n",
    "**2. SÄ±nÄ±f BazlÄ± DetaylÄ± Analiz**\n",
    "ğŸ”´ Negative (Olumsuz)\n",
    "Precision: 0.68\n",
    "Recall: 0.56\n",
    "F1: 0.61\n",
    "- Yorum:\n",
    "GerÃ§ek olumsuz yorumlarÄ±n yalnÄ±zca %56â€™sÄ± yakalanabilmiÅŸ.\n",
    "%44â€™Ã¼ positive veya neutralâ€™a kaymÄ±ÅŸ.\n",
    "Precision makul, ama recall dÃ¼ÅŸÃ¼k â†’ model negatifleri kaÃ§Ä±rÄ±yor.\n",
    "Operasyonel kullanÄ±mda (mÃ¼ÅŸteri ÅŸikÃ¢yetlerini yakalama) risklidir.\n",
    "\n",
    "ğŸŸ¡ Neutral (NÃ¶tr) â€” En ZayÄ±f SÄ±nÄ±f\n",
    "Precision: 0.25\n",
    "Recall: 0.35\n",
    "F1: 0.29\n",
    "Bu kritik bir zayÄ±flÄ±k:\n",
    "Model nÃ¶tr tahmin yaptÄ±ÄŸÄ±nda, bunun yalnÄ±zca %25â€™i doÄŸru.\n",
    "GerÃ§ek nÃ¶tr yorumlarÄ±n %65â€™i yanlÄ±ÅŸ sÄ±nÄ±fa gidiyor.\n",
    "Model nÃ¶tr kavramÄ±nÄ± Ã¶ÄŸrenememiÅŸ.\n",
    "Fiilen modelin 3 sÄ±nÄ±flÄ± deÄŸil, 2.3 sÄ±nÄ±flÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± sÃ¶ylemek mÃ¼mkÃ¼n.\n",
    "\n",
    "ğŸŸ¢ Positive (Olumlu)\n",
    "Precision: 0.90\n",
    "Recall: 0.88\n",
    "F1: 0.89\n",
    "\n",
    "-Yorum:\n",
    "Pozitif sÄ±nÄ±f Ã§ok gÃ¼Ã§lÃ¼.\n",
    "Model bÃ¼yÃ¼k Ã¶lÃ§Ã¼de â€œher ÅŸeyi pozitif gÃ¶rmeâ€ eÄŸiliminde.\n",
    "Bu sÄ±nÄ±f, accuracyâ€™yi yapay biÃ§imde yukarÄ± Ã§ekiyor.\n",
    "\n",
    "**3. Macro vs Weighted Ortalamalar**\n",
    "Macro Avg (F1 = 0.60)\n",
    "Her sÄ±nÄ±fa eÅŸit aÄŸÄ±rlÄ±k verir â†’ gerÃ§ek kaliteyi yansÄ±tÄ±r.\n",
    "â†’ DÃ¼ÅŸÃ¼k ve orta seviye arasÄ± bir performans.\n",
    "Weighted Avg (F1 = 0.78)\n",
    "Pozitif sÄ±nÄ±fÄ±n aÄŸÄ±rlÄ±ÄŸÄ± nedeniyle ÅŸiÅŸmiÅŸ.\n",
    "â†’ Ä°ÅŸ kararlarÄ± iÃ§in tek baÅŸÄ±na gÃ¼venilir deÄŸil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SonuÃ§ ve DeÄŸerlendirme\n",
    "\n",
    "| SÄ±nÄ±f | Precision | Recall | F1-Score | Destek |\n",
    "|---|---|---|---|---|\n",
    "| **Negative** | 0.68 | 0.56 | 0.61 | 642 |\n",
    "| **Neutral** | 0.25 | 0.35 | 0.29 | 437 |\n",
    "| **Positive** | 0.90 | 0.88 | 0.89 | 3016 |\n",
    "| **Accuracy** | â€” | â€” | **0.77** | 4095 |\n",
    "| **Macro Avg** | 0.61 | 0.59 | 0.60 | 4095 |\n",
    "\n",
    "**Bulgular:**\n",
    "- LSTM modeli pozitif yorumlarda yÃ¼ksek baÅŸarÄ± (F1 = 0.89) elde etmiÅŸtir; veri setinin bÃ¼yÃ¼k Ã§oÄŸunluÄŸunun pozitif olmasÄ± bu sÄ±nÄ±fÄ± gÃ¼Ã§lendirmektedir\n",
    "- NÃ¶tr sÄ±nÄ±f (F1 = 0.29) en zayÄ±f performansÄ± sergilemektedir â€” 3 puanlÄ± yorumlarÄ±n belirsiz sÄ±nÄ±r niteliÄŸi sÄ±nÄ±flandÄ±rmayÄ± zorlaÅŸtÄ±rmaktadÄ±r\n",
    "- Negatif yorumlarÄ±n %44'Ã¼ yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±lmaktadÄ±r; mÃ¼ÅŸteri ÅŸikayet tespiti aÃ§Ä±sÄ±ndan recall deÄŸeri (0.56) geliÅŸtirilmelidir\n",
    "- EarlyStopping ile aÅŸÄ±rÄ± Ã¶ÄŸrenme kontrolÃ¼ saÄŸlanmÄ±ÅŸ; val_accuracy Epoch 4'te 0.90'a ulaÅŸmÄ±ÅŸtÄ±r\n",
    "- BERT tabanlÄ± Ã¶n eÄŸitimli model veya daha dengeli veri seti ile nÃ¶tr ve negatif sÄ±nÄ±f F1 skoru artÄ±rÄ±labilir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
